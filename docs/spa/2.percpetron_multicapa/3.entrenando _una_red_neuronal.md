# Algoritmo Backpropagation

Este proceso está relacionado con el **aprendizaje supervisado**, que como hemos visto antes, es el hecho de tomar el error entre una salida esperada y la salida obtenida por la red neuronal, de esta manera se pueden ajustar los pesos y bias entre las capas para acercarse de la mejor manera a la respuesta correcta

## Costo de un sistema

Cuando hablamos de _costo_ primero debemos recordar que es el error; que resulta ser simplemente la diferencia entre las salidas obtenida y esperadas. En una red neuronal, expandimos esta definición mas allá; _costo_ es el error acumulado para un conjunto grande de datos (usados para entrenar una red neuronal)

## Conceptos básicos

Backpropagation es un algoritmo que altera los pesos de una red neuronal, propagando el error de la salida del sistema hacia atrás, de ahí el nombre.

Debes tener en cuenta que esta es una implementación bastante simple del algoritmo, dicho esto, la meta principal es tener un entendimiento básico de cómo funciona, de esta manera tendrás una mejor oportunidad de entender herramientas de machine-learning más robustas.

## Como funciona

Miremos el esquema de una red neuronal bastante simple, donde están solo las neuronas de la capa escondida y de salida:

![esquema_nn_simple](/docs/img/simple_nn_backpropagation1.jpg)

La principal diferencia entre la formula con _&Delta;W_ que calculábamos previamente en [aprendizaje supervisado](docs/spa/1.perceptron/2.aprendizaje_supervisado.md) es que para **Backpropagation** lo que realmente necesitamos hacer, es determinar que peso es el que tiene más influencia en el error, ya que ahora debemos calcular los errores independientes para cada neurona escondida.

Si es que observamos un diagrama completo, podemos ver realmente a que errores nos referíamos anteriormente.

![diagrama_completo](/docs/img/complete_nn_backpropagation1.jpg)

Como vemos, tenemos que calcular _err<sub>o<sub>1</sub></sub>, err<sub>h<sub>1</sub></sub>_ y _err<sub>h<sub>2</sub></sub>_

Estos dos últimos errores _err<sub>h<sub>1</sub></sub>_ y _err<sub>h<sub>2</sub></sub>_ serán usados para alterar los pesos entre la capa escondida y, de entrada, con el algoritmo de _descenso en gradiente_

Así que ahora podemos calcular los errores de la siguiente manera:

El error en la capa de salida es el mismo que habíamos revisado antes, _err<sub>o<sub>1</sub></sub> = y<sub>known<sub>1</sub></sub> - y<sub>guess<sub>1</sub></sub>_

Ahora podemos calcular _err<sub>h<sub>1</sub></sub>_ y _err<sub>h<sub>2</sub></sub>_ como porciones de _err<sub>o<sub>1</sub></sub>_, así que ahora tenemos:

_err<sub>h<sub>1</sub></sub> = ( <sup>w<sub>1</sub></sup> &frasl; <sub>( w<sub>1</sub>+w<sub>2</sub> )</sub> )* err<sub>o<sub>1</sub></sub>_

Y

_err<sub>h<sub>2</sub></sub> = ( <sup>w<sub>2</sub></sup> &frasl; <sub>( w<sub>2</sub>+w<sub>1</sub> )</sub> )* err<sub>o<sub>1</sub></sub>_

Aun así, este caso es uno particular, con una sola neurona de salida, ¿qué es lo que sucede cuando tenemos múltiples neuronas de salida o múltiples capas escondidas?

![esquema_completo_2](/docs/img/complete_nn_backpropagation2.jpg)

Ahora tenemos:

_err<sub>o<sub>1</sub></sub> = y<sub>known<sub>1</sub></sub> - y<sub>guess<sub>1</sub></sub>_

_err<sub>o<sub>2</sub></sub> = y<sub>known<sub>2</sub></sub> - y<sub>guess<sub>2</sub></sub>_

Con esta definición, podemos calcular el error de la capa de salida para un numero arbitrario de neuronas de salida, solo debemos seguir incrementando el número de errores individuales (_err<sub>o<sub>n</sub></sub> = y<sub>known<sub>n</sub></sub> - y<sub>guess<sub>n</sub></sub>_).

Por otra parte, para los errores de la capa oculta, ahora tenemos que tomar en cuenta la mayor cantidad de conexiones que existen al tener más neuronas en la capa de salida.

Lo hacemos de esta manera:

_err<sub>h<sub>1</sub></sub> = ( <sup>w<sub>11</sub></sup> &frasl; <sub>( w<sub>11</sub>+w<sub>12</sub> )</sub> )* err<sub>o<sub>1</sub></sub> + ( <sup>w<sub>21</sub></sup> &frasl; <sub>( w<sub>21</sub>+w<sub>22</sub> )</sub> )* err<sub>o<sub>2</sub></sub>_

Y para _err<sub>h<sub>2</sub></sub>_

_err<sub>h<sub>2</sub></sub> = ( <sup>w<sub>12</sub></sup> &frasl; <sub>( w<sub>12</sub>+w<sub>11</sub> )</sub> )* err<sub>o<sub>1</sub></sub> + ( <sup>w<sub>22</sub></sup> &frasl; <sub>( w<sub>22</sub>+w<sub>12</sub> )</sub> )* err<sub>o<sub>1</sub></sub>_

Podemos generalizar esta fórmula para cualquier número de neuronas escondidas y de salida, de la siguiente manera:

![formula general](/docs/img/general_hidden_err_formula.jpg)

Con _n_ siendo la cantidad de neuronas escondidas y _w_ la variable que representa a los pesos de las conexiones.
