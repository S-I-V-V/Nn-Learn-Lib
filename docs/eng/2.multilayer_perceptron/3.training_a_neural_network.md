# Backpropagation Algorithm

This process ties to **Supervised learning**, which as we have discussed it before, is the process in which the neural network learns by looking at the error between the desired and computed outputs, so it can adjust its weights and biases to better match the correct answer.

## Defining cost

To talk about the _cost_ of a system, we need to have a clear definition of error, which is the difference between the expected and produced outputs. In a neural network, we expand this definition further; _cost_ is the cumulative error for a large data set (used for training the neural network).

## Basics

Backpropagation is an algorithm that changes the weights of a neural network, propagating the error from the output layer backwards, hence the name.

Have in mind that this is a simplistic implementation of this algorithm; the main goal is to learn how it works, for you to have a better understanding of more powerful machine-learning tools.

## How it works

Let us look at a simple neural network scheme, where we only look at the hidden and output layers:

![simple_nn_scheme](/docs/img/simple_nn_backpropagation1.jpg)

The key difference from the _&Delta;W_ formula that we calculated before in [supervised learning](/docs/eng/1.perceptron/2.supervised_learning.md) is that for **Backpropagation** we really need to define which weight is responsible for the _error_, as we need to calculate independent errors for all the hidden layer neurons.

If we go to a full diagram, we can really see where we need to calculate these errors.

![complete_nn_scheme](/docs/img/complete_nn_backpropagation1.jpg)

As we see, we need to calculate _err<sub>o<sub>1</sub></sub>, err<sub>h<sub>1</sub></sub>_ and _err<sub>h<sub>2</sub></sub>_

This last two errors _err<sub>h<sub>1</sub></sub>_ and _err<sub>h<sub>2</sub></sub>_ will be used to tweak the weights between the input and hidden layers, with the _gradient descent_ algorithm.

Therefore, this is how the errors are computed:

The error at the output layer is the same as before,  _err<sub>o<sub>1</sub></sub> = y<sub>known<sub>1</sub></sub> - y<sub>guess<sub>1</sub></sub>_

Now we can calculate _err<sub>h<sub>1</sub></sub>_ and _err<sub>h<sub>2</sub></sub>_ as a portion of _err<sub>o<sub>1</sub></sub>_, now we have:

_err<sub>h<sub>1</sub></sub> = ( <sup>w<sub>1</sub></sup> &frasl; <sub>( w<sub>1</sub>+w<sub>2</sub> )</sub> )* err<sub>o<sub>1</sub></sub>_

And

_err<sub>h<sub>2</sub></sub> = ( <sup>w<sub>2</sub></sup> &frasl; <sub>( w<sub>2</sub>+w<sub>1</sub> )</sub> )* err<sub>o<sub>1</sub></sub>_

However, this is a particular case, with one output neuron, what happens if we have multiple output neurons or multiple hidden layers?

![complete_nn_scheme_2](/docs/img/complete_nn_backpropagation2.jpg)

Now we have:

_err<sub>o<sub>1</sub></sub> = y<sub>known<sub>1</sub></sub> - y<sub>guess<sub>1</sub></sub>_

_err<sub>o<sub>2</sub></sub> = y<sub>known<sub>2</sub></sub> - y<sub>guess<sub>2</sub></sub>_

Moreover, this holds for any number of output neurons we have, we just keep incrementing the number of individual errors (_err<sub>o<sub>n</sub></sub> = y<sub>known<sub>n</sub></sub> - y<sub>guess<sub>n</sub></sub>_).

Nevertheless, for the hidden layer errors, now we have to take into consideration the larger amount of connections.

Like this:

_err<sub>h<sub>1</sub></sub> = ( <sup>w<sub>11</sub></sup> &frasl; <sub>( w<sub>11</sub>+w<sub>12</sub> )</sub> )* err<sub>o<sub>1</sub></sub> + ( <sup>w<sub>21</sub></sup> &frasl; <sub>( w<sub>21</sub>+w<sub>22</sub> )</sub> )* err<sub>o<sub>2</sub></sub>_

And for _err<sub>h<sub>2</sub></sub>_

_err<sub>h<sub>2</sub></sub> = ( <sup>w<sub>12</sub></sup> &frasl; <sub>( w<sub>12</sub>+w<sub>11</sub> )</sub> )* err<sub>o<sub>1</sub></sub> + ( <sup>w<sub>22</sub></sup> &frasl; <sub>( w<sub>22</sub>+w<sub>12</sub> )</sub> )* err<sub>o<sub>1</sub></sub>_

We could generalize this formula, for any number of hidden neurons and output neurons like this:

![scheme formula](/docs/img/general_hidden_err_formula.jpg)

With _n_ being the hidden neuron index and _w_ being the weights variable.
